{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "* Zipf's law: $ f*r = k $, where $f$ is frequency, $r$ is rank, $k$ is a constant\n",
    "* Even in a large corpus, there will be many infrequent words\n",
    "\n",
    "# Distributional semantics\n",
    "## Vector semantics\n",
    "* Model words with vectors, aka embeddings\n",
    "* Shorter windows - more syntactic representation\n",
    "* Longer windows - more semantic representation\n",
    "* **PMI**: Is context word **informative** about a target word?\n",
    "    * $ P(x,y) = \\large{\\frac{P(x,y)}{P(x)P(y)}} $\n",
    "    * Replace negatives with 0\n",
    "    * Biased toward infrequent events \n",
    "        * Raise prob. of rare words by $\\alpha = .75$\n",
    "        * Add-k smoothing\n",
    "* **tf.idf**: Combine term frequency + inverse document frequency\n",
    "    * $ w_{i,j} = tf_{i,j} * log(\\frac{N}{df_i})$\n",
    "    * $N$ = # docs, $df_i$ is # of docs with word\n",
    "* **Cosine similarity**: $ cos(vec{v},\\vec{w}) = \\frac{\\vec{v} \\cdot \\vec{w}}{|\\vec{v}||\\vec{w}|} $\n",
    "    \n",
    "\n",
    "## Word sense disambiguation\n",
    "* **Zeugma** combines distinct senses of a word in an uncomfortable way\n",
    "* Same word, different senses:\n",
    "    * Homonyms: Unrelated (financial bank vs river bank)\n",
    "    * Polysemes: Related but distinct (financial bank vs blood bank vs tree bank)\n",
    "    * Metonyms: \"Stand-in\" (\"Washington\" instead of \"the US gov't\")\n",
    "* **Cohen's Kappa**: Quantify disagreement between human annotators\n",
    "    * $ K = \\large{\\frac{Pr(a) - Pr(e)}{1-Pr(e)}} $\n",
    "    * a = actual agreement, e = chance agreement\n",
    "* **Lesk's algorithm**: Count overlapping words between glosses & context\n",
    "\n",
    "### WSD as supervised classification\n",
    "* Precision: % of selected items that are correct\n",
    "* Recall: % of correct items that are selected\n",
    "* $ F = \\large{\\frac{(\\beta^2 + 1)PR}{\\beta^2P+R}} $\n",
    "\n",
    "## Perceptron\n",
    "* $ f(x) = sign(w^Tx + b) $\n",
    "* Update rule: if $sign(\\hat{y}) \\neq sign(y)$ update weights:\n",
    "    * $w_d = w_d + yx_d$ for all $ 1...D$\n",
    "    * $b = b + y$\n",
    "* $X$ is the **feature vector**\n",
    "* $w,b$ are **parameters**\n",
    "* $MaxIter$ is a **hyperparameter**\n",
    "* Voted perceptron: $\\hat{y} = sign( \\sum_{k=1}^Kc^{(k)}sign(w^{(k)}\\cdot \\hat{x} + b^{(k)}))$\n",
    "* Averaged perceptron: $\\hat{y} = sign((\\sum_{k=1}^Kc^{(k)}w^{(k)})\\cdot \\hat{x} + \\sum_{k=1}^Kc^{(k)}b^{(k)})$\n",
    "* Converges after $\\frac{R^2}{\\gamma^2} $ for margin $\\gamma$\n",
    "\n",
    "## Logistic regression\n",
    "* Sigmoid: $\\sigma(z) = \\large{\\frac{1}{1+e^{-z}}}$\n",
    "* $P(y=1) = \\sigma(w^Tx + b)$\n",
    "* $L_{CE}(w,b) = -[ylog\\sigma(w^Tx + b) + (1-y)log(1-\\sigma(w^Tx + b))]$\n",
    "    * aka: $-[ylog\\hat{y} + (1-y)log(1-\\hat{y})]$\n",
    "  \n",
    "### Gradient descent\n",
    "* Minimizes loss by steps in the opposite direction of the gradient\n",
    "* $\\frac{\\delta L_{CE}(w,b)}{\\delta w_j} = [\\sigma(w^Tx+b)-y]x_j$\n",
    "* $\\theta = \\theta - \\eta g$\n",
    "\n",
    "### Multiclass LR\n",
    "* Softmax: $softmax(z_i) = \\large{\\frac{e^z_i}{\\sum_{j=1}^k e^z_j}}$ for class $ 1 \\leq i \\leq k $\n",
    "* $P(y = c|x) = \\large{\\frac{e^{w_c^Tx + b_c}}{\\sum_{j=1}^k e^{w_j^Tx + b_j}}} $\n",
    "* $L_{CE}(\\hat{y},y) = -\\sum_{k=1}^K 1\\{y=k\\}log p(y=k|x)$, where $1\\{\\} = 1$ if true, and $0$ otherwise\n",
    "* $\\frac{\\delta L_{CE}(w,b)}{\\delta w_k} = -(1\\{y=k\\}-\\large{\\frac{e^{w_k^Tx + b_k}}{\\sum_{j=1}^k e^{w_j^Tx + b_j}}})x_k$\n",
    "\n",
    "## N-gram language models\n",
    "* Markov assumption: Approximate context history by last few words only\n",
    "* Bigram: $P(w_i|w_1w_2...w_{i-1}) \\cup P(w_i| w_{i-1})$\n",
    "* Generally insufficient because language has long-distance dependencies\n",
    "* $ P(w_i|w_{i-1}) = \\large{\\frac{count(w_{i-1}, w_i)}{count(w_{i-1})} }$\n",
    "* Smoothing:\n",
    "    * Sparse stats, generalize better\n",
    "    * LaPlace (add-1) smoothing: Add one to all counts\n",
    "        * Adjusted counts: $ c^*(w_{n-1}w_n) = \\large{\\frac{[c(w_{n-1}w_n)+1]*c(w_{n-1})}{C(w_{n-1})+V}}$\n",
    "    * Stupid backoff: Use less context\n",
    "        * $S(w_i|w_{i-k+1}^{i-1}) = \\large{\\frac{count(w_{i-k+1}^i)}{count(w_{i-k+1}^{i-1})}}$ if $count(w_{i-k+1}^i) > 0$,\n",
    "            $0.4S(w_i|w_{i-k+2}^{i-1})$ otherwise\n",
    "        * $S(w_i) = \\large{\\frac{count(w_i)}{N}}$\n",
    "* Can use < UNK > token for unknown words\n",
    "\n",
    "### Evaluating language models\n",
    "* **Perplexity**: $PP(W) = P(w_1w_2...w_N)^{-\\frac{1}{N}} = [\\prod_{i=1}^N P(w_i|w_1...w_{i-1})]^{-\\frac{1}{N}}$ \n",
    "\n",
    "## Neural network language model\n",
    "* Represent words as one-hot vectors\n",
    "* Probabilistic classifier to compute prob. of a word given n prev words\n",
    "* Error: Same as multiclass LR\n",
    "    * Corpus level: $error(\\lambda) = -\\sum_{E in corpus} log P_\\lambda (E) $\n",
    "    * Word level: $-log P_\\lambda (e_t|e_1...e_{t-1})$\n",
    "* Learns word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
