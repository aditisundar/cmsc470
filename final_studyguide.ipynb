{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense word embeddings { [x](http://www.cs.umd.edu/class/fall2019/cmsc470/slides/slides_10.pdf) }\n",
    "* Easier to use as features\n",
    "* Generalize better \n",
    "* Capture synonymy better\n",
    "* **word2vec**:\n",
    "    * Train classifier on binary prediction task - is $w$ likely to show up near \"apricot\"?\n",
    "    * Implicitly learns embeddings\n",
    "    * Skip-gram algorithm\n",
    "        * Target + neighbor = positive samples, random negative samples\n",
    "        * Logistic regression classifier - weights are embeddings \n",
    "        * Model for a pair of words $t,c$:  \n",
    "            $ P(+|t,c) = \\frac{1}{1+e^{-t \\cdot c}} $  \n",
    "            $ P(-|t,c) = 1 - P(+|t,c) $  \n",
    "        * Skip-gram for context words  \n",
    "        $P(+|t,c_{1:k}) = \\prod_{i=1}^k \\frac{1}{1+e^{-t \\cdot c_i}} $  \n",
    "        $log P(+|t,c_{1:k}) = \\sum{i=1}^k log \\frac{1}{1+e^{-t \\cdot c_i}} $  \n",
    "        * Training:\n",
    "            * Assume a context window\n",
    "            * For each positive example, create $k$ negative random examples (noise words)\n",
    "            * Can pick noise word $w$ according to $P_\\alpha(w) = \\frac{count(w)^\\alpha}{\\sum_w count(w)^\\alpha}$, $\\alpha = .75$\n",
    "            * Maximize $\\sum_{(t,c)\\in +} log P(+|t,c) + \\sum_{(t,c)\\in -}log P(-|t,c) $ (max prob of neg pairs being labeled as neg, max prob of pos pairs being labeled as pos)\n",
    "            * Throw away classifier & keep embeddings\n",
    "* Limitations:\n",
    "    * Multiple senses not captured\n",
    "    * Cosine similarity doesn't help with antonyms vs synonyms\n",
    "    * Reflect bias\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine translation { [x](http://www.cs.umd.edu/class/fall2019/cmsc470/slides/slides_11.pdf) }\n",
    "* History\n",
    "    * Rule-based --> statistical --> neural\n",
    "    * Right now, MT good for: \n",
    "        * Understanding content\n",
    "        * Communication\n",
    "        * Making content available in other languages\n",
    "* Challenges: Ambiguity, word order\n",
    "* Evaluation\n",
    "    * Adequacy: Conservation of meaning\n",
    "    * Fluency\n",
    "    * BLEU:\n",
    "        * $n$-grams of size 1 to 4\n",
    "        * $min(1, \\frac{out\\_len}{ref\\_len})(\\prod_{i=1}^4 precision_i)^{\\frac{1}{4}}$\n",
    "        * Computed over entire corpus\n",
    "        * Drawbacks: All words equally weighted, scores meaningless\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNs { [x](http://www.cs.umd.edu/class/fall2019/cmsc470/slides/slides_12.pdf) }\n",
    "* Can capture LD dependencies\n",
    "* Params:\n",
    "    * $W_{mh}: |H| * |V|$ ($V$ = embedding size)\n",
    "    * $W_{hh}: |H| * |H|$ \n",
    "    * $W_{hs}: |V| * |H|$\n",
    "* Update eqns:\n",
    "    * $m_t = M_{.,e_{t-1}} $\n",
    "    * $h_t = tanh(W_{mh}m_t + W_{hh}h_{t-1} + b_n) $ aka $RNN(m_t, h_{t-1})$\n",
    "    * $p_t = softmax(W_{hs}h_t + b_s)$\n",
    "* Training\n",
    "    * Online: Update weights for each training example\n",
    "    * Batch: Accumulate gradient over all examples & update at the end\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural machine translation { [x](http://www.cs.umd.edu/class/fall2019/cmsc470/slides/slides_13.pdf) }\n",
    "* $\\hat{E} = argmax_E P(E|F;\\theta)$\n",
    "* Encoder-decoder model\n",
    "    * $m_t^{(f)} = M_{.,f_t}^{(f)}$\n",
    "    * $h_t^{(f)} = RNN^{(f)}(m_t^{(f)}, h_{t-1}^{(f)})$\n",
    "    * $m_t^{(e)} = M_{.,e_{t-1}}^{(e)}$\n",
    "    * $h_t^{(e)} = RNN^{(e)}(m_t^{(e)}, h_{t-1}^{(e)})$\n",
    "    * $p_t^{(e)} = softmax(W_{hs}h_t^{(e)} + b_s)$\n",
    "* Search:\n",
    "    * Ancestral sampling: Randomly generate words one by one\n",
    "    * Greedy search: Pick highest prob. word at each time step\n",
    "        * Often generates easy words first\n",
    "        * Prefers multiple common words to rare words\n",
    "    * Beam search: Consider b total top hypotheses at each time step (instead of single top)\n",
    "* Bidirectional encoder\n",
    "    * $\\overrightarrow{h}_t^{(f)} = \\overrightarrow{RNN}^{(f)}(m_t^{(f)}, \\overrightarrow{h}_{t-1}^{(f)})$\n",
    "    * $\\overleftarrow{h}_t^{(f)} = \\overleftarrow{RNN}^{(f)}(m_t^{(f)}, \\overleftarrow{h}_{t+1}^{(f)})$\n",
    "    * $h_0^{(e)} = tanh(W_{\\overrightarrow{f}e}\\overrightarrow{h}_{|F|} + W_{\\overleftarrow{f}e}\\overleftarrow{h}_{1} + b_e)$ (Add forward & backward hidden to match input dim)\n",
    "* Attention mechanism { [x](http://www.cs.umd.edu/class/fall2019/cmsc470/slides/slides_14.pdf) }\n",
    "    * Decoding - use attention weights to perform linear combination of source sentence word vectors\n",
    "    * Let $H^{(f)} $ be the concat of all encoder outputs $ [\\overleftarrow{h}_j^{(f)}; \\overrightarrow{h}_j^{(f)}]$\n",
    "    * Let context vector $c_t = H^{(f)}\\alpha_t$\n",
    "    * $\\alpha_t$: Weights between 0 and 1\n",
    "    * $h_t^{(e)} = \\textrm{enc}([\\textrm{embed}(e_{t-1}); c_{t-1}], h_{t-1}^{(e)})$\n",
    "    * $a_{t,j} = \\textrm{attn_score}(h_j^{(f)}, h_t^{(e)}) $\n",
    "        * Dot product: $h_j^{(f)} \\cdot h_t^{(e)}$\n",
    "            * No extra params, hidden states must be same size\n",
    "        * Bilinear: $h_j^{(f)} W_a h_t^{(e)}$\n",
    "            * More params, but dims can be different\n",
    "        * Multi-layer perceptron (Bahdanau): $w_{a2}^{\\intercal}\\textrm{tanh}(W_{a1}[h_t^{(e)};  h_j^{(f)}])$\n",
    "            * Dims can be diff, less new params\n",
    "    * $\\alpha_t = softmax(a_t) $\n",
    "    * $p_t^{(e)} = softmax(W_{hs}[h_t^{(e)}; c_t] + b_s)$\n",
    "    * Can help interpret/illustrate translation decisions\n",
    "    * Help insert translations for OOV words\n",
    "* Addressing length bias (default models gen short sentences)\n",
    "    * Prior prob. on sentence length: $\\hat{E} = argmax_E log P(|E||F|) + log P(E|F)$\n",
    "    * Normalize by sentence length:  $\\hat{E} = argmax_E log \\frac{P(E|F)}{|E|}$\n",
    "* Works well only with high resources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subwords, multi-task, multilingual, semi-supervised ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging & sequence labeling { [x](http://www.cs.umd.edu/class/fall2019/cmsc470/slides/slides_19.pdf) }\n",
    "* Parts of speech to know:\n",
    "    * Noun, verb, adjective\n",
    "    * Adverb: Modify verbs/adjectives\n",
    "    * Prepositions: Spatial/temporal relations (*on* the shelf) - used with noun phrases\n",
    "    * Particles: Like prepositions, but with verbs (find *out*, turn *over*)\n",
    "    * Determiners: Articles (a, an, the), that, this, many, such\n",
    "    * Pronouns: Refer to entities/persons\n",
    "    * Conjunctions: \n",
    "        * Coordinating: Equal status (and, or)\n",
    "        * Subordinating: Unequal status (after, while, that)\n",
    "* POS tagging\n",
    "    * Uses Penn Treebank tagset\n",
    "    * Difficult due to ambiguity\n",
    "    * Perceptron: sequence of tokens x ==> sequence of tags y\n",
    "        * $ w \\leftarrow w + \\phi(x,y) - \\phi(x,\\hat{y}) $\n",
    "        * $ \\hat{y} = \\textrm{argmax}_{\\hat{y} \\in Y(x)} w \\cdot \\phi(x,\\hat{y})$\n",
    "    * Features (constant wrt input length):\n",
    "        * Unary: Between $x$ and a single label in $y$ ( # of times word $w$ has been labeled with tag $l$ for all words and tags )\n",
    "        * Markov: Between adjacent labels in $y$ ( # times tag $l$ adjacent to $l'$ in output for tag pairs $l, l'$\n",
    "    * Viterbi algorithm for argmax { [x](http://www.cs.umd.edu/class/fall2019/cmsc470/slides/slides_20.pdf) }: \n",
    "        * $l^{th}$ word, label $k$: $\\alpha_{l,k} = max_{\\hat{y}_{1:l-1}} w \\cdot \\theta_{1:l}(x, \\hat{y} \\circ k)$\n",
    "        * Output space becomes $lk^2$\n",
    "        * Features must *decompose over input* - can't look at predictions ahead or two timestamps back\n",
    "    * Hamming loss - more nuanced than 0-1 loss\n",
    "        * Minimize $l^{(Ham)}(y, \\hat{y}) = \\sum_{l=1}^L 1\\{y_l \\neq \\hat{y}_l\\}$\n",
    "* Information extraction - detecting named entities (BIO labeling scheme: B-PER, I-PER, O, B-ORG, I-ORG, etc)\n",
    "* Integer Linear Programming { [x](http://www.cs.umd.edu/class/fall2019/cmsc470/slides/slides_21.pdf) }\n",
    "    * Optimization of the form $max_z a \\cdot z $ subj to linear constraints on $z$\n",
    "    * Markov feature labeling as ILP\n",
    "        * $z_{l, k', k} = 1\\{l = k$ & $ l-1 = k'\\}$\n",
    "        * $a_{l,k',k} = w \\cdot \\theta_l(x,(...,k',k))$\n",
    "        * $z_{l,k',k} \\in \\{0,1\\} $\n",
    "        * For a given $l$, there is one active $z$\n",
    "        * $z$'s are internally consistent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntax, grammar & parsing { [x](http://www.cs.umd.edu/class/fall2019/cmsc470/slides/slides_21.pdf) }\n",
    "* Dependency trees: Connected, acyclic, single-head\n",
    "* Can help NLP tasks by resolving ambiguity & generalizing (LD dependencies)\n",
    "* Transition-based dependency parsing { [x](http://www.cs.umd.edu/class/fall2019/cmsc470/slides/slides_22.pdf) }\n",
    "    * Config: stack, buffer, set of arcs\n",
    "    * Arc standard parsing:\n",
    "        * SHIFT: Remove word from buffer, push on stack\n",
    "        * LA: 1st word on stack ==> 2nd, remove 2nd\n",
    "        * RA: 2nd word on stack ==> 1st, remove 1st\n",
    "        * Reqs:\n",
    "            * ROOT cannot have incoming arcs\n",
    "            * LA can't be applied when ROOT is 2nd element\n",
    "            * LA, RA reqire 2 elements in stack\n",
    "        * Weakness: Right dependents cannot be attached to head till all their dependents have been attached\n",
    "    * Training features for oracle\n",
    "         * Config + action taken\n",
    "         * Word forms, POS\n",
    "    * Arc eager parsing { [x](http://www.cs.umd.edu/class/fall2019/cmsc470/slides/slides_23.pdf) }\n",
    "         * SHIFT: Remove word from buffer, push on stack\n",
    "         * RA: Word on stack ==> word on buffer, move buffer head to stack\n",
    "         * LA: Word on buffer ==> word on stack, pop stack\n",
    "    * Properties:\n",
    "        * Soundness: Projective (no crossing arcs)\n",
    "            * Projective arc: If there is a path from head ==> every word between head and dependent\n",
    "        * Complete: There is a translation sequene that can generate a projective dep. forest\n",
    "        * Deal with non projectivity:\n",
    "            * Transform non-projective tree into projective (so it's reversible)\n",
    "            * Add new transitions\n",
    "* Dependency forest: single head, acyclic, not connected\n",
    "* Graph-based dependency parsing:\n",
    "    * Find best directed spanning tree of multi-digraph that captures all poss. dependencies\n",
    "    * Arc-factored model: Weight of graph can be factored as sum/product of weights of arcs\n",
    "    * Chu-Liu-Edmonds algorithm:\n",
    "        * Find highest scoring incoming arc for each vertex\n",
    "            * If this is a tree, we've found MST\n",
    "            * Else:\n",
    "                * Identify cycle and **contract**:\n",
    "                    * Recalculate arc weights into & out of cycle\n",
    "                    * Outgoing arcs = max of outgoing arc over all vertices in cycle\n",
    "                    * Incoming arc weights = weight of best spanning tree that includes head of incoming arc & all nodes in cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
